%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Nome: Jullyana Vycas                                          %
% Disciplina: Recuperação da Informação                         %
%                                                               %
% Esse trabalho trata-se de um estudo do artigo Learning to     %
% Rank with Selection Bias in Personal Search, de autoria de    %
% Xuanhui Wang, Michael Bendersky, Donald Metzler e Marc Najork %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

% Para língua portuguesa
\usepackage[utf8]{inputenc}

% Algoritmos
\usepackage{algorithm} 
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% Importar PDF Gráfico
\usepackage{graphicx}
\usepackage{pdfpages}

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\def\UrlFont{\rmfamily}

\begin{document}
\mainmatter              % start of a contribution
%
\title{Learning-to-Rank em Busca Personalizada}
%
\titlerunning{Learning-to-Rank em Busca Personalizada}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Jullyana Vycas}
%
\authorrunning{Jullyana Vycas} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Jullyana Vycas}
%
\institute{Universidade Federal do Rio de Janeiro, Rio de Janeiro RJ, Brasil,\\
\email{jullytta.ufrj@gmail.com},\\ WWW home page:
\texttt{linkedin.com/in/jullytta}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Esse trabalho trata-se de um estudo das propostas do artigo
\textit{Learning to Rank with Selection Bias in Personal Search}, incluindo
uma implementação parcial do algoritmo Learning-To-Rank e experimentos
envolvendo os modelos vetorial e probabilístico.

\keywords{learning-to-rank, personal search, BM25, machine learning}
\end{abstract}
%
\section{Introdução}
Novas tecnologias ligadas a inteligência artificial e aprendizado de máquina estão cada vez mais presentes na área
de recuperação da informação, formando os chamados sistemas de recuperação inteligentes \cite{chen}. 
No lugar de especialistas no domínio que manualmente definem a relevância de um documento, entram algoritmos capazes 
de aprender a ranquear. Contudo, esse conhecimento não surge espontâneamente: é preciso obter um conjunto de dados
significativo para servir de exemplo para o algoritmo, ou seja, alimentá-lo.

Aprendemos em sala de aula que pode-se obter feedback dos usuários através de seus cliques. Se apresentamos um ranking 
como resultado de uma consulta, o documento que for acionado pelo usuário (ou seja, aquele que for clicado) provavelmente
é relevante. Portanto, podemos utilizar a avaliação do próprio usuário para descobrir empiricamente a relevância de um 
documento perante uma consulta. Evidentemente, precisamos de usuários fazendo essas buscas e verificando os documentos 
recuperados. Para grandes buscadores na Web, este não é um problema. O buscador Google, por exemplo, lida com
mais de 40.000 consultas por segundo \cite{google:search:statistics}. Cada uma dessas consultas gera dados preciosos
que servirão como exemplo para um algoritmo de aprendizado de máquina.

Contudo, o que acontece no caso em que o domínio é mais restrito? Para buscas dentro de documentos pessoais, sejam eles
e-mails ou arquivos, como descobrimos quais documentos são relevantes? Não podemos nos basear nas milhares de pesquisas
semelhantes que foram feitas anteriormente: os documentos recuperados são privados e diferentes para cada usuário.

Nas próximas seções, nos aprofundaremos no algoritmo Learning-To-Rank em sua forma utilizada para buscas na Web, fazendo 
experimentos sobre o mesmo, simulando uma busca sobre domínio particular. Em seguida, estudaremos as propostas apresentadas 
por Want et al. \cite{wang:bendersky:metzlet:najork} que adaptam o algoritmo para melhor se adequar a buscas personalizadas,
observando as diferentes tendências que podem influenciar a coleta dos dados.


\section{Learning-to-Rank}
Dada uma consulta $q$ e seu conjunto de resultados ${x_1,\dots,x_n}$, o objetivo do algoritmo
é encontrar uma função de ranqueamento $f(x)$ que minimize a perda total.

A definição de perda varia de acordo com a implementação. Neste trabalho, utilizamos uma função de perda par-a-par
simples, sugerida como modelo clássico pelo artigo estudado.

Para cada par $x_i$ e $x_j$ onde $x_i$ é mais relevante que $x_j$, temos:

\begin{equation}
perda_{par} = max(0, f(x_j) - f(x_i))^2
\end{equation}

A perda da função $f$ para com a consulta $q$, ou seja, $perda(f, q)$, é a soma da perda par-a-par para todo 
$x_i, x_j$ indicado acima. Dessa forma, estamos penalizando funções que possuem pares fora de ordem.
Além disso, a perda par-a-par é proporcional à distância entre os ranks.

Digamos que temos um conjunto finito de consultas que o usuário pode fazer, $Q$. 
Selecionamos um subconjunto $U$ de consultas, como amostra aleatória, para avaliar a função $f$.
Assim, a perda total é:

\begin{equation}
perda_{total}(f) = \frac{1}{|U|}\times \sum_{q \in U} perda(f, q)
\end{equation}

Ou seja, uma média aritmética das perdas para cada consulta $q$.

Note que para sermos capazes de calcular a perda apresentada, precisamos conhecer de antemão a ordem decrescente de 
relevância  dos documentos para cada uma das consultas. Vamos discutir como abordar esse problema na próxima seção. 
Por hora, vamos considerar que temos um ``ranking ideal'' contendo essa informação.

\begin{algorithm}
  \caption{Perda Total}
  \begin{algorithmic}[1]
    \REQUIRE conjunto de consultas U, ranking ideal para cada consulta R, função f(x)
    \ENSURE perda total de f
    \STATE $perda_{total} \Leftarrow 0$
    \FOR{$q$ em $U$}
      \STATE $perda_{consulta} \Leftarrow 0$
      \FORALL{$x_i, x_j$ onde $x_i$ aparece antes de $x_j$ em $R$}
	\STATE $perda_{par} \Leftarrow max(0, f(x_j) - f(x_i))^2$
	\STATE $perda_{consulta} \Leftarrow perda_{consulta} + perda_{par}$
      \ENDFOR
      \STATE $perda_{total} \Leftarrow perda_{total} + perda_{consulta}$
    \ENDFOR
    \STATE $perda_{total} \Leftarrow perda_{total} \times \frac{1}{|U|}$
  \end{algorithmic}
\end{algorithm}

\newpage

Uma vez que definimos nossa avaliação de $f$, é possível aprender qual uma função ótima. Por exemplo, aproveitando a 
implementação feita em sala do ranqueamento BM25, podemos variar os parâmetros $b$ e $K_1$ até encontrar seus 
valores que minimizam a perda total. Assim, ``aprendemos'' uma função de rankeamento para consultas futuras.


\section{Obtendo Relevância Através de Cliques}
Como mencionado anteriormente, para aplicar o algoritmo descrito na seção anterior, precisamos saber de antemão a ordem
de relevância dos documentos. Para descobrí-la, utilizamos o feedback indireto dos usuários, através de cliques.

Vamos começar considerando a situação na qual o usuário fez uma consulta em um buscador web e clica em diversos resultados.
Será que podemos considerá-los relevantes para a consulta? Qual é mais relevante, o que foi clicado primeiro?
Todos os outros documentos não clicados são, então, irrelevantes?

Para responder estas perguntas, precisamos considerar as chamadas \textit{tendências}. O usuário não avalia a relevância
de cada documento apresentado independentemente; o ranqueamento influencia na decisão de clicar ou não em um resultado.
Existem vários tipos diferentes de tendências. A mais comum é a tendência de posição, ou ainda tendência de confiança:
um usuário tende a clicar em resultados com maior rank, tanto por confiar no SRI, quanto pela sua posição.
De fato, estudos mostram que apenas os dois primeiros resultados recebem grande atenção, que cai rapidamente quando 
avaliamos os cinco resultados seguintes \cite{thorsten:granka:pan:hembrooke:gay}.

Outras tendências existem: a tendência de qualidade, na qual a chance do usuário clicar em um resultado
depende da qualidade relativa dele em relação aos outros; a tendência de atração, na qual a aparência do resultado 
(e.g. texto em negrito) influencia nos cliques realizados \cite{yue:patel:roehrig}.


\section{Mudanças Propostas pelo Artigo}

Agora vamos considerar a situação levantada pelo artigo: a busca personalizada. Anteriormente, nós podíamos usar o feedback
que recebemos de um usuário para melhorar os resultados de uma determinada consulta para todos os usuários. Infelizmente,
não podemos usar o mesmo artifício da ``sabedoria das massas'' no caso da busca personalizada. Como os documentos buscados
são todos pessoais (desde e-mails a arquivos), o domínio é diferente para cada usuário. Temos, portanto, poucos cliques para
nos basear em, o que dificulta o cálculo da relevância de um documento.

Para o artigo estudado, o cenário era ainda mais específico: após clicar em um documento, o usuário é levado para outra página.
Ou seja, a cada consulta, pode haver no máximo um clique. Assim, o conjunto de dados disponível é um grupo de consultas feitas 
anteriormente que tiveram cliques. As consultas que não levaram à cliques são descartadas no cálculo de relevância, pois não 
temos nenhum feedback sobre estas.

Surge um novo problema de tendência: a tendência de seleção. Como selecionamos um conjunto $U$ comparativamente pequeno de 
consultas para basear nosso algoritmo, precisamos adicionar novos parâmetros ao cálculo da perda total para chegar a 
resultados mais precisos.

O artigo usa ponderação de propensão inversa para calcular um peso para cada consulta. Se chamamos de $S$ o conjunto de 
consultas que possuem cliques, temos que a propensão de uma consulta é a probabilidade desta consulta estar em $S$, ou
seja, $P(S)$. A ponderação também se utiliza da probabilidade da consulta estar no grupo selecionado $U$, $P(U)$.
A nova perda total é dada por:

\begin{equation}
perda_{total}(f) = \frac{1}{|S|}\times \sum_{q \in S} \frac{P(U)}{P(S)} \times perda(f, q)
\end{equation}

Tais probabilidades são calculadas empiricamente. O artigo propõe formas mais completas e complexas que fogem do escopo do
trabalho. A estimativa grosseira utilizada foi que $P(S) = \frac{|S|}{|T|}$ e $P(U) = \frac{|U|}{|T|}$, onde T é o número
total de consultas diferentes que podemos fazer no sistema. Consideramos um conjunto limitado de termos que podem ser
utilizados para tornar essa abordagem possível.


\section{Conjuntos de Dados}
Como o objetivo é trabalhar com um conjunto de dados particular, coleções com julgamento de relevância explícito não são
abundantes. De fato, apesar de existirem muitos sites que disponibilizam conjuntos de dados interessantes, como o site da
TREC \cite{trec}, nenhum deles se encaixava bem no propósito desse trabalho. O mais próximo encontrado foi o 
conjunto de dados de e-mails Eron \cite{eron}.

Construímos, portanto, nosso próprio conjunto de dados, baseado no conjunto Eron. O conjunto de dados simula uma caixa de
e-mails de uma mulher chamada Jane Doe, jane.doe@exemplo.com. Cada documento é um email, representado por uma string.
Essas strings tem a seguinte estrutura:

\begin{verbatim}
<email>
  <de></de>
  <para></para>
  <assunto></assunto>
  <categoria></categoria>
  <mensagem></mensagem>
  <anexo></anexo>
</email>
\end{verbatim}

Durante o pré-processamento de dados, foram removidas stopwords comuns da língua portuguesa. Os atributos da estrutura 
dos documentos também foram considerados stopwords. Portanto, não foram indexadas as palavras 'email', 'de',
'para', 'assunto', 'categoria', 'mensagem' e 'anexo'.

O conjunto de dados completo, junto da lista de stopwords e separadores, se encontra em: 
\url{https://github.com/jullytta/recinfo/blob/master/Trabalho2/src/entrada.sce}


\section{Experimentos e Resultados}
As implementações foram feitas em Scilab. O código desenvolvido especificamente para este trabalho pode ser visualizado em 
\url{https://github.com/jullytta/recinfo/tree/master/Trabalho2/src}.
Note que muitas funções escritas em sala de aula foram reaproveitadas. Elas se encontram no mesmo repositório, porém em um
diretório diferente: \url{https://github.com/jullytta/recinfo/tree/master/Scilab}.

Inicialmente foi implementada uma função de perda que tinha como entrada um ranking ideal e uma função de ranqueamento.
Contudo, para se adequar melhor ao escopo do artigo, foi decidido que o único dado disponível para inferir a relevância dos
documentos seria qual deles foi clicado após a consulta. Com isso, a função de perda teve de sofrer alterações para se 
adequar à falta de informações.

De forma a simplificar a implementação, decidiu-se que o documento que recebeu o clique era mais relevante, porém não se 
faria distinção entre os outros. Portanto, a função de perda considera todos os pares $x_i, x_j$ nos quais $x_i$ é um 
documento clicado e $x_j$ é um documento não clicado.

Essa função de perda inicial, que segue o clássico Learning-to-Rank apresentado no artigo, foi usada para 
calcular a perda total, tanto para o modelo vetorial quanto para o modelo probabilístico, a cada consulta. 
Em especial, como o modelo probabilístico tem parâmetros ajustáveis, valores diferentes foram testados de forma
a encontrar a perda total mínima. Curiosamente, os valores menores de b tiveram perda total significativamente menor do que
valores recomendados de b (como 0.75).

Em seguida, prosseguimos implementando a alteração proposta pelo artigo. Com poucos dados sem muita credibilidade, as
probabilidades estimadas foram ruins, tanto para $P(S)$ quanto para $P(U)$. Não só isso, mas todas as consultas acabavam
por ter probabilidades muito semelhantes, o que fez com que os valores da perda total fossem equivalentes aos encontrados
com o algoritmo padrão. Como podemos observar na Fig. \ref{graf:linear}, o resultado foi linear e desanimador.

Procurando resultados mais interessantes, resolvemos considerar aleatória a probabilidade de uma consulta ter recebido um
clique, $P(S)$. Assim, algumas consultas receberam alta probabilidade de clique (talvez sejam mais frequentes, ou seus 
resultados são melhores) e outras receberam baixa probabilidade de clique (menos frequentes ou não possuíam o resultado 
desejado).

Os resultados podem ser observados na Fig. \ref{graf:rand}. Alguns valores de $b$ geraram uma perda muito grande, mas o 
valor ótimo deixou de ser $0$ e passou a ser $0.15$. Ainda estranhamente baixo, contudo um pouco mais animador.

Finalmente, ocorreu a ideia de usar os valores de propensão encontrados empiricamente no artigo para avaliar a função
de perda. Por posição no ranking, tem-se $[0.4, 0.24, 0.18, 0.16]$, que significa que quanto menor a posição no rank,
menor as chances de se conseguir um clique. Infelizmente, mesmo utilizando esses valores, o resultado foi tão parecido com 
a Fig. \ref{graf:linear} que seria redundante adicioná-lo ao relatório.

\begin{figure}[ht]
  \caption{Probabilidades estimadas e estáveis.}
  \label{graf:linear}
  \centering
  \includegraphics[page=1,width=.65\textwidth]{b_x_perda_linear_crop.pdf}
\end{figure}

\begin{figure}[ht]
  \caption{P(S) aleatória.}
  \label{graf:rand}
  \centering
  \includegraphics[page=1,width=.65\textwidth]{b_x_perda_rand_crop.pdf}
\end{figure}

\pagebreak


\section{Conclusões}
A busca personalizada tem muitas limitações: a dificuldade em encontrar conjuntos de dados para estudo, a
esparsidade de feedback do usuário, a seleção de consultas significativas para o treinamento no caso de aprendizado
de máquina, entre outras. Precisamos de um modelo mais complexo para superar essas dificuldades.

Nossos experimentos foram fortemente baseados no modelo probabilístico BM25 e mostram que a função padrão do Learning-to-Rank
escolhe valores muito baixos para o parâmetro b. Esses resultados foram contra intuitivos. A implementação, em si, não 
saiu como era esperado.


% Bibliografia
\begin{thebibliography}{6}
%

\bibitem {wang:bendersky:metzlet:najork}
Wang, Xuanhui, Bendersky, Michael, Metzler, Donald, Najork, Marc: Learning to Rank with Selection Bias in Personal Search.
SIGIR (2016). \url{https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45286.pdf}

\bibitem {chen}
Chen, Hsinchun: Machine Learning for Information Retrieval: Neural Networks, Symbolic Learning, and Genetic Algorithms.
Journal of the American Society for Information Science (1995). \url{https://www.marilia.unesp.br/Home/Instituicao/Docentes/EdbertoFerneda/chen27.pdf}

\bibitem {google:search:statistics}
Internet Live Stats: Google Search Statistics.
Acesso em 13/06/2017. \url{http://www.internetlivestats.com/google-search-statistics/}

\bibitem {thorsten:granka:pan:hembrooke:gay}
Joachims, Thorsten, Granka, Laura, Pan, Bing, Hembrooke, Helene, Gay, Geri: Accurately Interpreting Clickthrough Data as Implicit
Feedback.
SIGIR (2005). \url{https://www.cs.cornell.edu/people/tj/publications/joachims_etal_05a.pdf}

\bibitem {yue:patel:roehrig}
Yue, Yisong, Patel, Rajan, Roehrig, Hein: Beyond Position Bias: Examining Result Attractiveness as
a Source of Presentation Bias in Clickthrough Data.
IW3C2 (2010). \url{https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36363.pdf}

\bibitem {trec}
Text REtrieval Conference (TREC): Data.
Acesso em 13/06/2017. \url{http://trec.nist.gov/data.html}

\bibitem {eron}
Eron Email Dataset.
Acesso em 13/06/2017. \url{https://www.cs.cmu.edu/~./enron/}

\end{thebibliography}
\end{document}
